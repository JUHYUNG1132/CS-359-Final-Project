{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Process\n",
    "---\n",
    " In this part, it will \n",
    " * read data inside 'fall2024data/'\n",
    " * convert data into one dataframe\n",
    " * change the name of features\n",
    " * drop rows which containing Nan or Inf value\n",
    " * save plot of features to 'Featrues_plot/'\n",
    " * save Analystic data to 'Analysis/'\n",
    " * save processed data to csv in 'Datasets/'\n",
    "\n",
    "    'Dataset.csv'           - whole dataset of traffic\\\n",
    "    'BENIGN.csv'            - set of data labeled 'BENIGN'\\\n",
    "    'DoS_GoldenEye.csv'     - set ofdata labeled 'DoS_GoldenEye'\\\n",
    "    'DoS_Hulk.csv'          - set of data labeled 'DoS_Hulk'\\\n",
    "    'DoS_Slowhttptest.csv'  - set of data labeled 'DoS_Slowttptest'\n",
    "    \n",
    " ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "---\n",
    "This part will load data from the folder and concatenate them into one DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import time\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logged(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        start_time_formatted = time.strftime('%H:%M:%S', time.localtime(start_time))\n",
    "        print(f\"[{start_time_formatted}] Function '{func.__name__}' start.\")\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        end_time_formatted = time.strftime('%H:%M:%S', time.localtime(end_time))\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"[{end_time_formatted}] Function End, Time Elapsed: {execution_time:.4f}Sec\")\n",
    "        \n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logged\n",
    "def get_Dset(fpath:str)->pd.DataFrame:\n",
    "    _ids = list()\n",
    "    try:\n",
    "        # get csv files\n",
    "        for csvfile in glob.glob(f'{fpath}/*.csv'):\n",
    "            print('{:30s}'.format(csvfile), 'found')\n",
    "            _ids.append(pd.read_csv(csvfile, sep=','))\n",
    "\n",
    "        # get json files\n",
    "        for jsonfile in glob.glob(f'{fpath}/*.json'):\n",
    "            print('{:30s}'.format(jsonfile), 'found')\n",
    "            _ids.append(pd.read_json(jsonfile, lines=True))\n",
    "\n",
    "        # get parquet files\n",
    "        for pqfile in glob.glob(f'{fpath}/*.parquet'):\n",
    "            buff = pq.read_table(pqfile)\n",
    "            print('{:30s}'.format(pqfile), 'found')\n",
    "            _ids.append(buff.to_pandas())\n",
    "\n",
    "        return pd.concat(_ids, ignore_index=True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception:', e)\n",
    "        return\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = get_Dset('fall2024data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "---\n",
    "In this part, data will be separated by its Label and processed to show some insight\n",
    "* Data types conversion\n",
    "* Data format conversion (cm to inches, etc.)\n",
    "* Identifying errors in data\n",
    "* Handling out-of-range and outlier data\n",
    "* Add any other transformations you find necessary.\n",
    "\n",
    "Also, Drop Label 'Heartbleed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logged\n",
    "def drop_Heartbleed(data:pd.DataFrame):\n",
    "    # Drop Label 'Heartbleed'\n",
    "    Hbd = (data.iloc[:,-1] == 'Heartbleed')\n",
    "    H_idx = Hbd[Hbd == True].index\n",
    "    print(H_idx.shape[0], 'items dropped')\n",
    "    df = data.drop(H_idx)\n",
    "    return df\n",
    "\n",
    "df = drop_Heartbleed(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Info about data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 61117 samples with 78 features and 1 label\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All of features are in numerical type, thus, we don't need to transform it.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.unique(df.iloc[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the name of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logged\n",
    "def strip_cols(df:pd.DataFrame)->pd.DataFrame:\n",
    "    # Some of features have confusing spaces in their name\n",
    "    cols = df.columns.to_list()\n",
    "    \n",
    "    for i in range(len(cols)):\n",
    "        print('{:30} ->'.format(cols[i]), end= ' ')\n",
    "        cols[i] = cols[i].strip()\n",
    "        print('{:30}'.format(cols[i]))\n",
    "        \n",
    "    return df.set_axis(cols, axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stripped = strip_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stripped.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows which contains Nan or Inf value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logged\n",
    "def drop_anomaly(df:pd.DataFrame)->pd.DataFrame:\n",
    "    # Convert Inf value into Nan\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Columns containing NaN value\n",
    "    print(df.isna().sum().sum(), 'items dropped')\n",
    "    \n",
    "    # 'DoS Hulk' and 'BENIGN' contains Nan or Inf value\n",
    "    print(np.unique(df.loc[(ids.count(axis=1) < df.shape[1]), :].to_numpy()[:,-1], return_counts=True))\n",
    "    return df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = drop_anomaly(df_stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENIGN = df_dropped.loc[df_dropped['Label'] == 'BENIGN']\n",
    "DoS_GoldenEye = df_dropped.loc[df_dropped['Label'] == 'DoS GoldenEye']\n",
    "DoS_Hulk = df_dropped.loc[df_dropped['Label'] == 'DoS Hulk']\n",
    "DoS_Slowhttptest = df_dropped.loc[df_dropped['Label'] == 'DoS Slowhttptest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting each features\n",
    "Just so watch distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@logged\n",
    "def show_fig(target:list):\n",
    "    for j in range(len(BENIGN.columns)-1):\n",
    "        fig, ax = plt.subplots(len(target), 1, constrained_layout=True)\n",
    "        fig.set_dpi(600)\n",
    "    \n",
    "        target_col = j\n",
    "        fig.suptitle(BENIGN.columns[target_col])\n",
    "    \n",
    "        for i in range(len(target)):\n",
    "            \n",
    "            ax[i].set_title(target[i].iloc[0,-1])\n",
    "            ax[i].scatter(range(target[i].shape[0]), \n",
    "                          target[i].iloc[:,target_col].to_numpy(),\n",
    "                          marker='x', \n",
    "                          s=[5 for _ in range(target[i].shape[0])])\n",
    "        \n",
    "        fig.savefig(f\"Features_plot/{j}_{BENIGN.columns[j].replace('/', '')}.jpeg\", dpi=600)\n",
    "        plt.close(fig)\n",
    "        print(f\"Features_plot/{j}_{BENIGN.columns[j].replace('/', '')}.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_fig([BENIGN, DoS_GoldenEye, DoS_Hulk, DoS_Slowhttptest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.describe().to_csv('Analysis/ids_describe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DoS_GoldenEye.describe().to_csv('Analysis/GoldenEye_describe.csv')\n",
    "DoS_Hulk.describe().to_csv('Analysis/Hulk_describe.csv')\n",
    "DoS_Slowhttptest.describe().to_csv('Analysis/Slowhttptest_describe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DoS_Slowhttptest.to_csv('Datasets/DoS_Slowhttptest.csv')\n",
    "DoS_Hulk.to_csv('Datasets/DoS_Hulk.csv')\n",
    "DoS_GoldenEye.to_csv('Datasets/DoS_GoldenEye.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.to_csv('Datasets/Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Part\n",
    "---\n",
    "In this part, It will\n",
    "* Read data from .csv file\n",
    "* Exploratory Data Analysis\n",
    "* Data Preprocessing\n",
    "* Feature Engineering\n",
    "* Model Selecting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Read Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "file_path = 'Datasets/Dataset.csv'\n",
    "data = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis (EDA)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Identify the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Unify the columns/features names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [col.strip() for col in data.columns]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Unique values in the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique class labels:\", data['Label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Checking for missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = data.isnull().sum()\n",
    "print(\"Missing data per column:\", missing_data[missing_data > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Columns with highly missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5  # 50% threshold\n",
    "high_missing = missing_data[missing_data / data.shape[0] > threshold]\n",
    "print(\"Columns with >50% missing data:\", high_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Univariate Analysis: Statistics and Boxplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statisctics\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the data\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for first numerical feature\n",
    "sns.boxplot(data=data.iloc[:, :-1])\n",
    "plt.title(\"Boxplot of numerical features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Bivariate Analysis: Correlation Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude non-numeric columns\n",
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_data.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix (Numeric Features Only)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort correlation matrix\n",
    "@logged\n",
    "def corr_sort(corr_matrix:pd.DataFrame, get_val=True)->pd.DataFrame:\n",
    "    \"\"\"sort correlation matrix \n",
    "\n",
    "    Args:\n",
    "        corr_matrix (pd.DataFrame): square correlation matrix\n",
    "\n",
    "    Returns:\n",
    "        corr_sorted(pd.DataFrame): sorted, unpacked correlation matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert correlation matrix to long-form DataFrame\n",
    "    unpacked = corr_matrix.unstack().reset_index()\n",
    "    unpacked.columns = ['Feature_1', 'Feature_2', 'Corr']\n",
    "    \n",
    "    # Remove self-correlations\n",
    "    unpacked = unpacked[unpacked['Feature_1'] != unpacked['Feature_2']]\n",
    "    \n",
    "    # Drop duplicate pairs (e.g., A-B and B-A are the same)\n",
    "    unpacked['Pair'] = unpacked.apply(lambda row: frozenset([row['Feature_1'], row['Feature_2']]), axis=1)\n",
    "    unpacked = unpacked.drop_duplicates(subset='Pair')\n",
    "    unpacked.drop(columns=['Pair'], inplace=True)\n",
    "    \n",
    "    # Sort by absolute correlation values in descending order\n",
    "    unpacked['Abs_Corr'] = unpacked['Corr'].abs()\n",
    "    corr_sorted = unpacked.sort_values(by='Abs_Corr', ascending=False)\n",
    "    \n",
    "    # Drop helper column\n",
    "    corr_sorted.drop(columns=['Abs_Corr'], inplace=True)\n",
    "    corr_sorted.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(corr_sorted.head(10))\n",
    "    \n",
    "    if(get_val):\n",
    "        return corr_sorted\n",
    "    else:\n",
    "        return corr_sorted[['Feature_1', 'Feature_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 correlated features\n",
    "top = corr_sort(corr_matrix, get_val=False)\n",
    "top_10_corr_features = top.head(10).to_numpy().flatten()\n",
    "top_10_corr_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Multivariate Analysis: Clustering with HDBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array for DBSCAN\n",
    "features = numeric_data.to_numpy()\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "hdbscan = HDBSCAN(min_samples=5000, store_centers='centroid', n_jobs=-1)\n",
    "cluster_res = hdbscan.fit_predict(features)\n",
    "\n",
    "# Visualize the clusters (using the first two numeric features)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(features[:, 0], features[:, 1], c=cluster_res, cmap='viridis', s=10)\n",
    "plt.title(\"HDBSCAN Clustering Visualization (First Two Features)\")\n",
    "plt.xlabel(numeric_data.columns[0])\n",
    "plt.ylabel(numeric_data.columns[1])\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze DBSCAN results\n",
    "unique_clusters = set(cluster_res)\n",
    "print(\"Unique clusters identified by DBSCAN:\", unique_clusters)\n",
    "print(\"Number of outliers (label = -1):\", list(cluster_res).count(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare cluster with actual label\n",
    "comparison_table = pd.crosstab(cluster_res, data['Label'])\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map cluster with label * manually *\n",
    "cluster_to_label_map = {-1: 'Others',0: 'DoS Slowhttptest', 1: 'DoS Hulk', 2: 'BENIGN', 3: 'DoS GoldenEye'}  # 예시 매핑\n",
    "mapped_clusters = [cluster_to_label_map[c] for c in cluster_res]\n",
    "\n",
    "# Score accuracy\n",
    "accuracy = accuracy_score(data['Label'], mapped_clusters) \n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "ConfusionMatrixDisplay.from_predictions(data['Label'], mapped_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result is positive,\n",
    "# HDBSCAN found some structures among features\n",
    "from sklearn.metrics import silhouette_score\n",
    "print('Silhouette Score:', silhouette_score(data.iloc[:,:-1], cluster_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some insights\n",
    "print(\"Cluster Labels:\", cluster_res)\n",
    "print(\"Probability of Membership:\", hdbscan.probabilities_)\n",
    "print(\"Centroids:\", hdbscan.centroids_)\n",
    "# This information can be used to determine correlations between labels and features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = numeric_data.quantile(0.25)\n",
    "Q3 = numeric_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Determine outliers\n",
    "outliers = ((numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "# Display results\n",
    "print(\"Number of outliers per column:\")\n",
    "print(outliers)\n",
    "\n",
    "print(\"Total outliers:\", outliers.sum(), 'among', numeric_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IQR Calculation\n",
    "---\n",
    "The interquartile range (IQR) is calculated based on numeric columns only:\n",
    "\n",
    "IQR = 𝑄3−𝑄1\n",
    "\n",
    "IQR = 𝑄3−𝑄1\n",
    "\n",
    "Outliers are values outside:\n",
    "\n",
    "Lower Bound = 𝑄1−1.5×IQR\n",
    "\n",
    "Upper Bound = 𝑄3+1.5×IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justifications\n",
    "---\n",
    "EDA Techniques: These methods provide a comprehensive overview of the dataset.\n",
    "\n",
    "For example, correlation matrices help identify relationships between features, while boxplots reveal outliers.\n",
    "\n",
    "Tools: Seaborn is ideal for creating advanced visualizations with minimal code,\n",
    "\n",
    "We used HDBSCAN from Scikit-learn for clustering since our dataset is not scaled\n",
    "\n",
    "also Scikit-learn is a reliable library for clustering and machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Data Preprocessing\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Duplicate Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Splitting Dataset and Unify Dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.select_dtypes(include=[np.number]).astype(np.float64)  # Select Numeric features and unify datatypes\n",
    "                                                                # Ensure 'Label' or any categorical columns are excluded from features\n",
    "y = data['Label']  # Target variable (categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "print(pd.unique(y))\n",
    "y = label_encoder.fit_transform(y)\n",
    "print(pd.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train, Validation, and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set split (70%)\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, train_size=0.7, random_state=42, stratify=y)\n",
    "\n",
    "# Validation, Test set split (15% each)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rest, y_rest, test_size=0.5, stratify=y_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Handling Missing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean from the training data only\n",
    "train_mean = X_train.mean()\n",
    "\n",
    "# Fill missing values using the training data mean\n",
    "X_train.fillna(train_mean, inplace=True)\n",
    "X_valid.fillna(train_mean, inplace=True)\n",
    "X_test.fillna(train_mean, inplace=True)\n",
    "print('Missing in X_train:', X_train.isnull().sum().sum())\n",
    "print('Missing in X_valid:', X_valid.isnull().sum().sum())\n",
    "print('Missing in X_test:', X_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Verify the processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_train shape:\", X_valid.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train unique labels:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.2 Justifications\n",
    "---\n",
    "Preprocessing Choices\n",
    "\n",
    "* Duplicate Removal:\n",
    " Ensures that redundant records do not bias the model or distort statistical properties of the dataset.\n",
    " Reduces computational overhead during training.\n",
    "\n",
    "* Encoding Categorical Data:\n",
    " LabelEncoder is used to transform categorical target labels into numeric values required for machine learning models.\n",
    " Ensures that class labels are represented consistently without introducing unnecessary dimensions.\n",
    "\n",
    "* Splitting the Dataset:\n",
    " A 70-30 split ensures sufficient data for both training and testing while balancing computational feasibility.\n",
    " This is a standard practice for general model validation.\n",
    "\n",
    "* Handling Missing Data:\n",
    " Filling missing values with the mean is computationally efficient and maintains the dataset's distribution for numeric features.\n",
    " Ensures that no test statistics are leaked during preprocessing, as means are calculated separately for training and testing sets.\n",
    "\n",
    "* Technology Choices:\n",
    " Pandas: For efficient preprocessing operations like deduplication and handling missing values.\n",
    " Scikit-learn: For splitting datasets and encoding categorical variables, ensuring compatibility with downstream modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Nutshell\n",
    "\n",
    "* X_train, X_valid, X_test: Splited datasets\n",
    "* X__scaled: Standardized datasets\n",
    "* X__reduced_var: Datasets of selected features, standardized\n",
    "* X__reduced_RF_imp: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logged\n",
    "def train_RF(n_estimators=200, xtrain=X_train, \n",
    "             ytrain=y_train, xtest=X_test, ytest=y_test)->RandomForestClassifier:\n",
    "    \n",
    "    \"\"\"train RandomForest Classifier and show metrics\n",
    "    I chose recall score since I think the less False Positive the better.\n",
    "\n",
    "    Args:\n",
    "        n_estimators (int, optional): Defaults to 100.\n",
    "        xtrain (_type_, optional): Defaults to X_train.\n",
    "        ytrain (_type_, optional): Defaults to y_train.\n",
    "\n",
    "    Returns:\n",
    "        RandomForestClassifier: trained model\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    rf_model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    rf_model.fit(xtrain, ytrain)\n",
    "    \n",
    "    # Predicting\n",
    "    res = rf_model.predict(xtest)\n",
    "    print(res)\n",
    "    \n",
    "    # Scoring\n",
    "    y_re = np.where(ytest >= 1, 1, ytest)\n",
    "    res = np.where(res >= 1, 1, res)\n",
    "    \n",
    "    # Plotting\n",
    "    ConfusionMatrixDisplay.from_predictions(y_re, res)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(y_re, res))\n",
    "    print('Recall:', recall_score(y_re, res))\n",
    "    \n",
    "    return rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference model without any tweak\n",
    "# Fit model\n",
    "rf_model = train_RF(200, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Mean of X_train_scaled:', X_train_scaled.mean())\n",
    "print('Mean of X_valid_scaled:', X_valid_scaled.mean())\n",
    "print('Mean of X_test_scaled:', X_test_scaled.mean())\n",
    "print('Std of X_train_scaled:', X_train_scaled.std())\n",
    "print('Std of X_valid_scaled:', X_valid_scaled.std())\n",
    "print('Std of X_test_scaled:', X_test_scaled.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Remove features with near-zero variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit VarianceThreshold on X_train only\n",
    "variance_filter = VarianceThreshold(threshold=0.01)  # Features with >1% variance\n",
    "X_train_reduced_var = variance_filter.fit_transform(X_train_scaled)\n",
    "\n",
    "# Apply the same filter to X_valid and X_test\n",
    "X_valid_reduced_var = variance_filter.transform(X_valid_scaled)\n",
    "X_test_reduced_var = variance_filter.transform(X_test_scaled)\n",
    "\n",
    "variance_filter.feature_names_in_ = X_train.columns\n",
    "print('Selected Features:', variance_filter.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "rf_model_reduced_var = train_RF(200, X_train_reduced_var, y_train, X_valid_reduced_var, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = rf_model_reduced_var.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]  # Sort by importance\n",
    "\n",
    "# Select dominant 20 features\n",
    "num_features = 20\n",
    "sel_features = X_train.columns[indices[:num_features]]\n",
    "\n",
    "print(\"Selected Features:\", sel_features)\n",
    "\n",
    "# Reduce dataset\n",
    "X_train_reduced_RF_imp = X_train[sel_features]\n",
    "X_valid_reduced_RF_imp = X_valid[sel_features]\n",
    "X_test_reduced_RF_imp = X_test[sel_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RandomForest model\n",
    "rf_model_reduced_imp = train_RF(200, X_train_reduced_RF_imp, y_train, X_valid_reduced_RF_imp, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix on X_train_reduced\n",
    "correlation_matrix = np.corrcoef(X_train_reduced_var, rowvar=False)\n",
    "correlated_features = set()\n",
    "threshold = 0.9\n",
    "\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i + 1, correlation_matrix.shape[1]):\n",
    "        if abs(correlation_matrix[i, j]) > threshold:\n",
    "            correlated_features.add(j)\n",
    "            \n",
    "print(\"X_train shape before filtering:\", X_train_reduced_var.shape)\n",
    "print(\"X_valid shape before filtering:\", X_train_reduced_var.shape)\n",
    "print(\"X_test shape before filtering:\", X_test_reduced_var.shape)\n",
    "\n",
    "X_train_uncorrelated = np.delete(X_train_reduced_var, list(correlated_features), axis=1)\n",
    "X_valid_uncorrelated = np.delete(X_valid_reduced_var, list(correlated_features), axis=1)\n",
    "X_test_uncorrelated = np.delete(X_test_reduced_var, list(correlated_features), axis=1)\n",
    "\n",
    "# Final datasets\n",
    "print(\"X_train shape after filtering:\", X_train_uncorrelated.shape)\n",
    "print(\"X_valid shape after filtering:\", X_valid_uncorrelated.shape)\n",
    "print(\"X_test shape after filtering:\", X_test_uncorrelated.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with uncorrelated\n",
    "rf_model_uncorrelated = train_RF(200, X_train_uncorrelated, y_train, X_valid_uncorrelated, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO for feature selection\n",
    "lasso = LassoCV(random_state=42)\n",
    "lasso.fit(X_train_uncorrelated, y_train)\n",
    "\n",
    "# Select features with non-zero coefficients\n",
    "selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "X_train_lasso = X_train_uncorrelated[:, selected_features]\n",
    "X_valid_lasso = X_valid_uncorrelated[:, selected_features]\n",
    "X_test_lasso = X_test_uncorrelated[:, selected_features]\n",
    "\n",
    "print(\"X_train shape after filtering:\", X_train_lasso.shape)\n",
    "print(\"X_valid shape after filtering:\", X_valid_lasso.shape)\n",
    "print(\"X_test shape after filtering:\", X_test_lasso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with lasso\n",
    "rf_model_lasso = train_RF(200, X_train_lasso, y_train, X_valid_lasso, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = rf_model_lasso.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]  # Sort by importance\n",
    "\n",
    "# Select dominant 20 features\n",
    "num_features = 20\n",
    "sel_features = X_train.columns[indices[:num_features]]\n",
    "\n",
    "print(\"Selected Features:\", sel_features)\n",
    "X_train_reduced_RF_imp2 = X_train[sel_features]\n",
    "X_valid_reduced_RF_imp2 = X_valid[sel_features]\n",
    "X_test_reduced_RF_imp2 = X_test[sel_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_reduced_imp2 = train_RF(200, X_train_reduced_RF_imp2, y_train, X_valid_reduced_RF_imp2, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA for dimensionality reduction\n",
    "pca = PCA(n_components=5)  # Retain 5 principal components\n",
    "X_train_pca = pca.fit_transform(X_train_lasso)\n",
    "X_valid_pca = pca.transform(X_valid_lasso)\n",
    "X_test_pca = pca.transform(X_train_lasso)\n",
    "\n",
    "print(\"X_train shape after filtering:\", X_train_pca.shape)\n",
    "print(\"X_valid shape after filtering:\", X_valid_pca.shape)\n",
    "print(\"X_test shape after filtering:\", X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_pca = train_RF(200, X_train_pca, y_train, X_valid_pca, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# LDA for dimensionality reduction\n",
    "lda = LDA(n_components=3)  # Number of classes - 1\n",
    "X_train_lda = lda.fit_transform(X_train_lasso, y_train)\n",
    "X_valid_lda = lda.transform(X_valid_lasso)\n",
    "X_test_lda = lda.transform(X_test_lasso)\n",
    "\n",
    "print(\"X_train shape after filtering:\", X_train_lda.shape)\n",
    "print(\"X_valid shape after filtering:\", X_valid_lda.shape)\n",
    "print(\"X_test shape after filtering:\", X_test_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_lda = train_RF(200, X_train_lda, y_train, X_valid_lda, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Random Forest ?\n",
    "---\n",
    "* Ease of Use: Random Forest requires less hyperparameter tuning compared to XGBoost, making it quicker to implement for feature selection.\n",
    "* Feature Importance: Random Forest provides clear rankings of feature importance, which is directly used for selecting the top features.\n",
    "* Efficiency: It handles large datasets with many features effectively and is robust to overfitting when used for feature selection.\n",
    "* Interpretability: The feature importance scores are straightforward to interpret, unlike the complexity of interpreting gradient-boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logged\n",
    "def train_XG(n_estimators=200, xtrain=X_train, \n",
    "             ytrain=y_train, xtest=X_test, ytest=y_test)->XGBClassifier:\n",
    "    \n",
    "    \"\"\"train XGBoost Classifier and show metrics\n",
    "    I chose recall score since I think the less False Positive the better.\n",
    "\n",
    "    Args:\n",
    "        n_estimators (int, optional): Defaults to 100.\n",
    "        xtrain (_type_, optional): Defaults to X_train.\n",
    "        ytrain (_type_, optional): Defaults to y_train.\n",
    "\n",
    "    Returns:\n",
    "        XGBClassifier: trained model\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    xg_model = XGBClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    xg_model.fit(xtrain, ytrain)\n",
    "    \n",
    "    # Predicting\n",
    "    res = xg_model.predict(xtest)\n",
    "    print(res)\n",
    "    \n",
    "    # Scoring\n",
    "    y_re = np.where(ytest >= 1, 1, ytest)\n",
    "    res = np.where(res >= 1, 1, res)\n",
    "    \n",
    "    # Plotting\n",
    "    ConfusionMatrixDisplay.from_predictions(y_re, res)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(y_re, res))\n",
    "    print('Recall:', recall_score(y_re, res))\n",
    "    \n",
    "    return xg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model = train_XG(200, X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model_reduced_var = train_XG(200, X_train_reduced_var, y_train,\n",
    "                                X_valid_reduced_var, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = xg_model_reduced_var.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]  # Sort by importance\n",
    "\n",
    "# Select dominant 20 features\n",
    "num_features = 20\n",
    "sel_features = X_train.columns[indices[:num_features]]\n",
    "\n",
    "print(\"Selected Features:\", sel_features)\n",
    "X_train_reduced_XG_imp = X_train[sel_features]\n",
    "X_valid_reduced_XG_imp = X_valid[sel_features]\n",
    "X_test_reduced_XG_imp = X_test[sel_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model_reduced_imp = train_XG(200, X_train_reduced_XG_imp, y_train, X_valid_reduced_XG_imp, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model_uncorrelated = train_XG(200, X_train_uncorrelated, y_train, X_valid_uncorrelated, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model_lasso = train_XG(200, X_train_lasso, y_train, X_valid_lasso, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model_pca = train_XG(200, X_train_pca, y_train, X_valid_pca, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model_lda = train_XG(200, X_train_lda, y_train, X_valid_lda, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why XGBoost?\n",
    "---\n",
    "* High Performance: XGBoost often achieves superior accuracy due to its sophisticated boosting algorithm, making it ideal for complex datasets.\n",
    "* Handles Non-Linear Features: XGBoost excels at capturing intricate relationships between features, making it suitable for datasets with non-linear dependencies.\n",
    "* Feature Importance: Like Random Forest, XGBoost provides feature importance rankings but also supports advanced metrics like SHAP values for deeper interpretability.\n",
    "* Efficiency with Sparse Data: XGBoost is optimized for handling missing or sparse data, making it robust in real-world scenarios.\n",
    "* Customizability: It offers extensive hyperparameter tuning options, allowing for precise control over model behavior and better optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final datasets after feature engineering\n",
    "print(\"X_train_lasso shape:\", X_train_lasso.shape)\n",
    "print(\"X_test_valid shape:\", X_valid_lasso.shape)\n",
    "print(\"X_test_lasso shape:\", X_test_lasso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert processed arrays to DataFrames\n",
    "X_train_df = pd.DataFrame(X_train_lasso, columns=[f\"Lasso_{i+1}\" for i in range(X_train_lasso.shape[1])])\n",
    "X_valid_df = pd.DataFrame(X_valid_lasso, columns=[f\"Lasso_{i+1}\" for i in range(X_valid_lasso.shape[1])])\n",
    "X_test_df = pd.DataFrame(X_test_lasso, columns=[f\"Lasso_{i+1}\" for i in range(X_test_lasso.shape[1])])\n",
    "\n",
    "# Include target variable\n",
    "X_train_df['Label'] = y_train\n",
    "X_valid_df['Label'] = y_valid\n",
    "X_test_df['Label'] = y_test\n",
    "\n",
    "# Save to CSV\n",
    "X_train_df.to_csv(\"processed_X_train.csv\", index=False)\n",
    "X_valid_df.to_csv(\"processed_X_valid.csv\", index=False)\n",
    "X_test_df.to_csv(\"processed_X_test.csv\", index=False)\n",
    "\n",
    "print(\"Processed data saved to 'processed_X_train.csv', 'processed_X_valid.csv', and 'processed_X_test.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet for more efficient storage\n",
    "X_train_df.to_parquet(\"processed_X_train.parquet\", index=False)\n",
    "X_valid_df.to_parquet(\"processed_X_valid.parquet\", index=False)\n",
    "X_test_df.to_parquet(\"processed_X_test.parquet\", index=False)\n",
    "\n",
    "print(\"Processed data saved to 'processed_X_train.parquet', 'processed_X_test.parquet', and 'processed_X_test.parquet'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@logged\n",
    "def mass_train(name, model, xtrain, ytrain, xtest, yvalid, results):\n",
    "    print(f\"Training {model}...\")\n",
    "    model.fit(xtrain, ytrain)       # Use lasso-transformed data or top features\n",
    "    y_pred = model.predict(xtest)   # Predict on the test set\n",
    "    \n",
    "    y_class = np.where(yvalid >= 1, 1, yvalid)\n",
    "    y_pred = np.where(y_pred >= 1, 1, y_pred)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    accuracy = accuracy_score(y_class, y_pred)\n",
    "    precision = precision_score(y_class, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_class, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_class, y_pred, average=\"weighted\")\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Support Vector Machine\": SVC(kernel=\"linear\", random_state=42),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store performance metrics\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    mass_train(name, model, X_train_lasso, y_train, X_valid_lasso, y_valid, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to a DataFrame for easier comparison\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"Model Comparison:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example: Tuning Random Forest\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [5, 15, None],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBClassifier(), param_grid, cv=5\n",
    "                           , scoring=\"recall_weighted\", verbose=2)\n",
    "grid_search.fit(X_train_lasso, y_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Recall Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_model_final = XGBClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "xg_model_final.fit(X_train_lasso, y_train)\n",
    "y_pred = xg_model_final.predict(X_test_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = np.where(y_test >= 1, 1, y_test)\n",
    "y_pred = np.where(y_pred >= 1, 1, y_pred)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_class, y_pred)\n",
    "precision = precision_score(y_class, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_class, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_class, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_class, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs359",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
